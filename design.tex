
\section{Design and implementation}
Our main goal when designing \oursys was to preserve a standard \sockets API, while providing seamless zero-copy I/O support. A secondary goal is to eliminate costly system calls from the data path. 

We can classify the existing zero copy techniques into four categories:
\begin{enumerate}
    \item Dynamic remapping (e.g., tcp\_mmap, MSG\_ZEROCOPY\cite{desendmsg}, mikelangelo-project\cite{mikelangelo}).
    \item Kernel Bypass (e.g., DPDK, Netmap\cite{rizzo2012netmap}).
    \item Special/Limited use-case (splice, sendfile).
    \item Shared buffers (e.g, INSTANCE \cite{instance}).
\end{enumerate}
Out of the four categories only \#1 and \#4 allow for a generic use of standard \sockets API. Due to mainly security concerns examples that fit into category \#4 are the rarest.
While there are many examples to solutions that fall into category \#1 \cite{mikelangelo-empty,desendmsg}, but they usually discover that modifying virtual memory on the fly is computationally expensive. 

For these reasons we have decided that we should look into a Shared buffer solution. Our inspiration for a correct shared buffer solution comes from a similar problem on IOMMU security. DAMN \cite{markuze2018damn}, creates a memory allocator based on a pool of perpetually DMA mapped buffers, which are used for I/O by the Linux Network stack and device drivers. This solution effectively creates a \emph{secure} shared memory solution between the Server and the NIC.

We propose \oursys, a Memory Allocator for I/O, for the  exclusive use of the network stack. The \oursys allocator uses a pool of dedicated compound memory pages(i.e., \_\_GFP\_COMP). We adopt the allocation mechanics proposed in \cite{markuze2018damn}. I.e., the allocator is based on two known mechanisms, a page\_frag mechanism \cite{pagefrag} over 32KB buffers, and the buffers in turn are allotted by a magazine infrastructure \cite{bonwick2001magazines}. These pages then mapped to Read/Write \emph{once} to a privileged user-space process. That process is now able of perform zero-copy I/O.

\subsection{Shared buffer concerns.}
In early solutions proposed for zero-copy by shared static buffer, were considered dangerous because these shared pages can be exhausted and cannot be swapped out [add citation]. We contend that this is not a real concern for modern systems as systems with hundreds of GB are the norm. Key/Value applications (e.g., memcached, redis) expect their memory to be persistent in memory. Additionaly HPC applications that 

\subsection{Security}
Such a solution initially razes concerns about the security and stability of the system, as the process now \emph{seemingly} has access to sensitive kernel memory. 

\noindent\textbf{Driver Support.} We can allocate dedicated NIC RX rings for \oursys users. HW support\cite{flow_direct} can direct a single 5 tuple (or a defined group of 5 tuples). Limiting the shared buffers \emph{only} to this users data. The implementation of a driver support, is out of the scope of this work. 

\noindent\textbf{Kernel Security.} \oursys is integrated in such a way that the shared pages are only ever used by the Kernel to hold the I/O \emph{data} buffers and \emph{not} the meta data or any other kernel need. Namely, the process can only ever see the information it has written or data bound to user-space. In addition to the data, the process is privy to the transport headers as well; we assume the NIC supports Header/Data splitting\cite{hds} which can place the headers onto non-shared buffers.

\noindent\textbf{User Security.} By sharing all potential RX buffers \oursys exposes all traffic to a single observer.
Without driver support this limits the usefulness of \oursys to those cases when the user is trusted. 

\subsection{zero copy support for kernel sockets}
\subsection{Virtual Environment}
TODO: Virto does dynamic re-mapping, some papers also exist on kvm by the same people of "mikelangelo" \cite{mikelangelo}. Need to look at VMXNET3 code...
\subsection{Bifurcated I/O}
Related to FlexSC \cite{flexsc}, \\TODO:\\
1. Get ioring tests from dante732,\\
   kernel client, client, ioring , pktgen on VM.\\
2. rerun context switch tests?