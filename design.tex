\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{ktcp_z.pdf}
    \caption{1. \oursys shared memory buffers 2. shared io rings for exception-less system calls 3. A kernel thread executing I/O operations 4. A dedicated RX ring.
    [a] zero-copy send\_msg [b] zero-copy recvmsg [c] \oursys buffer in driver TX ring [d] \oursys buffers used by the device driver for RX}
    \label{fig:our_sys}
\end{figure} 

\section{Design and implementation}\label{sec:design}
Our main goal when designing \oursys was to preserve a standard \sockets API while providing a seamless zero-copy I/O support. A secondary goal is to eliminate costly system calls from the data path. However, we also needed to make sure that the security of the system is preserved.

%Out of the four categories only \#1 and \#4 allow for a generic use of standard \sockets API. Due to mainly security concerns examples that fit into category \#4 are the rarest.
%While there are many examples to solutions that fall into category \#1 \cite{mikelangelo-empty,desendmsg}, but they usually discover that modifying virtual memory on the fly is computationally expensive (Sec .\ref{sec:eval}). 

%For these reasons we have decided that we should explore a shared buffer solution. Our inspiration for a correct shared buffer implementation comes from a similar problem in IOMMU security, where the cost of dynamic remapping IOMMU address was resulting in poor performance. DAMN \cite{markuze2018damn}, creates a memory allocator over a pool of perpetually DMA mapped pages, which are used exclusively for I/O by the Linux Network stack and device drivers. This solution effectively creates a \emph{secure} shared memory solution between the Server and the NIC.

We propose \oursys, a Memory Allocator for I/O, used exclusively by a specific user-process and the kernel device drivers. 
We adopt the allocation mechanics proposed in DAMN\cite{markuze2018damn}. I.e., the allocator is based on two known mechanisms; a page\_frag mechanism \cite{pagefrag} over \size buffers, these buffers in turn allotted by a magazine allocator \cite{bonwick2001magazines}. This scheme allows for the efficient allocation of variable size buffers in the kernel. 
% this is needed to support variable sizes of MTU and HW offloads (e.g., HW GRO). 
We mark the metadata of these \oursys pages to indicate that they belong to \oursys. With this tag, anytime an I/O  memory page is freed in the kernel~(e.g., on TX completion), it will return to \oursys rather than to the kernel page allocator. 

To facilitate zero-copy in user-space, \oursys pages are mapped \emph{once} to the privileged user-space process's address space. The user-space then allocates from the kernel a batch of buffers for its use (i.e., zero-copy send). Another way the user-space process can get \oursys buffers is by performing zero-copy receive. The user-space process can return memory to the kernel via a exception-less mechanism described in Sec. \ref{sec:bifurcated}.
The \oursys buffers are depicted in Fig. \ref{fig:our_sys} as dashed boxes, same pages are used in TX and RX by the user (marked \textbf{[a]} and \textbf{[b]}) and TX/RX by the device driver (marked \textbf{[c]} and \textbf{[d]}). With zero-copy operations, the page's ownership is transferred between the user and kernel on each I/O operation or by explicitly allocating/freeing pages from/to the kernel.
%That process is now able to perform zero-copy I/O.
\subsection{Bifurcated I/O}\label{sec:bifurcated}
%A second issue that impacts user space I/O performance is the direct and indirect cost of system calls\cite{flexsc}.
To avoid the costly system calls, we delegate the I/O operation to a dedicated kernel thread (Fig. \ref{fig:our_sys} \textbf{(3)}) which will perform the I/O operation using kernel sockets \cite{ktcp}, this method has shown to be very efficient. For example a \texttt{send\_msg} system call is replaced with an I/O descriptor (i.e., \texttt{struct msghdr} and \texttt{int flags}) written to a shared memory ring buffer (Fig. \ref{fig:our_sys} \textbf{(2)}). This form of splitting the responsibility for performing I/O preservers the existing socket API, facilitates exception-less system calls, and allows for better parallel programming. Bifurcated I/O enables the separation of the application computations and the TCP computations to different CPU cores. Both zero-copy and standard send are supported; in standard mode, the sent buffer is copied to a new \oursys buffer before send. Supporting standard mode of send is beneficial for applications performing small I/O, e.g., 64B sends.


%In additional dedicated kernel threads are used to perform memory operations i.e., retrieving memory buffers back from the user.%RX: just poll the descriptor ring or sleep with sys call.
%TX: continuous poll. NAPI like execution, with amortized sys call, when not polling.


\subsection{Security}
Such a solution initially razes concerns about the system's security and stability, as the process now \emph{seemingly} has access to sensitive kernel memory. 

\noindent\textbf{Kernel Isolation.} \oursys is integrated in such a way that the shared pages are only ever used by the kernel to hold the I/O \emph{data} buffers and \emph{not} the metadata or any other kernel need. Namely, the process can only ever see the information it has written or data-bound to user-space. In addition to the data, the process is privy to the transport headers as well; we assume the NIC supports Header/Data splitting\cite{hds} which can place the headers onto non-shared buffers.

\noindent\textbf{Driver Support.} Modern NICs have the ability to create process dedicated rings~(e.g., Intel ADQ~\cite{adq}, Mellanox NICs~\cite{bifurcate}). HW support\cite{flow_direct} can direct a single 5 tuple (or a defined group of 5 tuples). Limiting the shared buffers \emph{only} to this users data. The dedicated RX ring shown in Fig. \ref{fig:our_sys} \textbf{(4)} is using \oursys pages (Fig. \ref{fig:our_sys} \textbf{[d]}). In Fig. \ref{fig:our_sys}, we also see the kernel buffer (marked as the opaque box) being used by the same TX ring as a \oursys page(Fig. \ref{fig:our_sys} \textbf{[c]}), no special care is needed for data separation. The implementation of a driver support, is out of the scope of this work. 

\noindent\textbf{User Isolation.} Isolating data access between I/O streams of different processes is achieved with application dedicated rings, and each process has its instance of \oursys. For virtual environments, the hypervisor can trivially be extended to support application dedicated rings. On older systems with NICs that do not support dedicated rings, \oursys can still be used if usage is restricted only to a privileged user (e.g., root, netdev group).    

\subsection{Shared buffer concerns.}
\noindent\textbf{Kernel Starvation.} The user process may hoard \oursys buffers without releasing them to the kernel.
In this case, the driver will revert to standard memory allocation and render the application unable to receive, while other process and kernel functionality will remain intact.


\noindent\textbf{Pinned pages.} Zero-copy solutions with shared static buffer were once considered dangerous because these shared pages can be exhausted and cannot be swapped out \cite{song2012performance,yamagiwa2005active}. We contend that this is not a real concern for modern systems as systems with abundant memory are the norm. Key/Value applications (e.g., Memcached, Redis) expect their memory to be persistent in memory. Additionally HPC applications, many\cite{top500} of which use RDMA, \texttt{register} (i.e., pin to memory) large memory regions that are then used for I/O.

\subsection{Zero Copy support for kernel sockets}
We expand the existing Linux TCP API with a \texttt{tcp\_read\_sock\_zcopy} for \texttt{RX} and add a new msg flag \texttt{SOCK\_KERN\_ZEROCOPY} for \texttt{tcp\_sendmsg\_locked} in \texttt{TX}. 
\noindent \textbf{RX.} We base our new function \texttt{tcp\_read\_sock\_zcopy} on existing infrastructure i.e., \texttt{tcp\_read\_sock}. It is used by \texttt{tcp\_splice\_read} to collect buffers from a socket without copying.

\noindent \textbf{TX.} Zero-copy infrastructure already exists in the form of MSG\_ZEROCOPY\cite{desendmsg}. When kernel memory is used for I/O, enabling zero-copy is trivial compared to zero-copy from userspace. The pages are already pinned in memory, and there is no need for a notification on \texttt{TX} completion. %The pages are reference counted and can be freed by the device driver completion handler.


\noindent \textbf{\texttt{do\_tcp\_sendpages}.}
Instead of modifying the behavior of \texttt{tcp\_sendmsg\_locked}, its also possible to use \texttt{do\_tcp\_sendpages}, which is used in splice. Ironically, \texttt{do\_tcp\_sendpages} accepts only one page fragment (i.e., \texttt{struct page}, size and offset) per invocation and does not work with a scatter-gather list, which \texttt{tcp\_sendmsg\_locked} supports.

%Related to FlexSC \cite{flexsc}, \\TODO:\\
%1. ~/memory\_trace/poller/\\
%2. rerun context switch tests?