\section{Introduction}

\sockets provide a convenient API for user-space I/O,  but network speeds have outstripped those of the CPU and memory in the past decades. Furthermore, while CPU and memory clock speeds have stagnated in the past decade, Ethernet speeds are growing steadily~\cite{roadmap}. Modern systems need multiple CPU cores to saturate a +10GB/s network link. The most significant performance overhead is memory copying, which can take a big part of the CPU cycles ~\cite{desendmsg}. Also, heavy copy operations can hurt other processes by "polluting" the shared L3 cache and putting additional pressure on the memory channels \cite{markuze2016true}. 

%The performance cost of memory copying is well known. 
Attempts to ameliorate these data moving costs have spawned numerous partial optimizations in the kernel (e.g., splice, \texttt{sednfile}, MSG\_ZEROCOPY, tcp\_mmap). However, A comprehensive solution is still missing, resulting in the adoption of kernel bypass technologies (e.g., KTCP, Netmap, XDP). Kernel bypass solutions eschew the rich networking infrastructure, developed and perfected in the kernel, leaving the developers with the need to re-develop existing infrastructure (e.g., ARP, IPv4, TCP, ICMP, IGMP). In this work, we add our solution to the myriad of zero-copy solutions proposed in the past 40 years, none providing all the needed elements, .i.e, a simple and generic API coupled with low overheads. The most significant hurdle for zero-copy networking is isolating processes form each other and from kernel memory. 

Unlike storage devices that enjoy zero-copy I/O support for many years, zero-copy for networking is a difficult problem. Due to the effectively random arrival pattern of network packets, it is impossible to know in advance which packet will arrive next and to what process it is intended. Due to the random arrival pattern, user memory could not be used in the receive flow. By contrast, in storage, I/O operations are issued by a local process, which provides the corresponding memory for that operation. Using process memory for I/O avoids any security issues as all I/O operations are performed without going through kernel memory.
We contend that the optimal solution has been overlooked due to concerns over memory isolation. With the advent of multi-queue NICs and large memory systems, these concerns are easily addressable. In this work, we show that existing network interface cards (NICs) already have the capability to facilitate the needed memory isolation.
%+100GbE era \cite{roadmap}.

%We draw inspiration from previous works on IOMMU security \cite{markuze2016true,markuze2018damn}. In that case, dynamic remapping was incompatible with performance and yet extensively used. 
We assert that the secure and performant solution for networking is a statically mapped memory region dedicated for network I/O. We demonstrate how a Memory Allocator for I/O (\oursys) approach provides generic zero-copy support for \sockets without compromising the security of the system. 
\oursys consists of a modified Linux kernel 5.4, in which we add I/O pages to the kernels memory management system. These I/O pages are solely used for network I/O, thus insulating I/O memory, accessible to the user from the rest of the kernel's memory. Additionally \oursys has a user-space library for managing I/O memory. We also provide a DPDK Poll Mode Driver (PMD), which can support multiple NIC types with a single \oursys PMD.
%We classify the existing solutions into five main categories(Sec .\ref{sec:design}) and show that the statically mapped shared buffer approach has remained relatively unexplored. With most recent techniques (e.g., \cite{xdp,dpdk,rizzo2012netmap,mikelangelo,desendmsg}) focusing instead on dynamic remapping and kernel bypass technologies.

To provide a fully overhead free I/O, we also work to eliminate the cost of system calls. In this case, we adopt the kernel thread approach proposed by FlexSC\cite{flexsc} and exemplified in \texttt{io\_uring}\cite{io-uring}. We show that shared memory buffers coupled with kernel threads provide a holistic solution for high-performance networking.
%We call our system \oursys, a dedicated Memory Allocator for I/O. 

\oursys is potentially beneficial for many VMware products like NSX and Velocloud. Additionally, \oursys is also extendable to ESX guest networking, potentially eliminating the performance gap between SR-IOV NICs and VMXNET3 para-virtual devices.