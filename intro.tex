\section{Introduction}
\sockets provide a convenient API for user-space I/O,  but network speeds have outstripped those of the CPU and memory in the past decades. Furthermore, while CPU and memory clock speeds have stagnated in the past decade, Ethernet speeds are growing steadily~\cite{roadmap}. Modern system need several CPU cores to saturate a +10GB/s network link. The most significant performance overhead stems from memory copying, which can take a big part of the CPU cycles ~\cite{desendmsg} and hurt other processes by "polluting" the shared L3 cache and putting additional pressure on the memory channels \cite{markuze2016true}. 

%The performance cost of memory copying is well known. 
Attempts to ameliorate these data moving costs have spawn numerous partial optimizations in the kernel (e.g., splice, sednfile, MSG\_ZEROCOPY, tcp\_mmap). Yet, A comprehensive solution is still missing, resulting in the adoption of kernel bypass technologies (e.g., KTCP, Netmap, XDP). Kernel bypass solutions eschew the rich networking infrastructure, developed and perfected in the kernel, leaving the developers with the need to re-develop existing infrastructure (e.g., ARP, IPv4, TCP, ICMP, IGMP). In this work, we add our solution to the myriad of zero-copy solutions proposed in the past 40 years, none providing all the needed elements, .i.e, a simple and generic API coupled with low overheads. The greatest hurdle for zero-copy networking is the problem of isolating processes form each other and from kernel memory. 

Unlike storage devices that enjoy zero-copy I/O support for many years, zero-copy for networking is a difficult problem. Due to the effectively random arrival pattern of network packets, it is impossible to know in advance which packet will arrive next and to what process it is intended and thus kernel memory is used in the receive flow. By contrast, in storage, each I/O operation is issued by a local process which also provides the corresponding memory for that operation. Which, avoids any security issues as all I/O operations are performed with the process memory.
We contend that the optimal solution has been overlooked due to concerns over memory isolation. With the advent of multi-queue NICs and large memory systems, these concerns are easily addressable. In this work we show that existing network interface cards (NICs) already have the capability to facilitate the needed memory isolation.
%+100GbE era \cite{roadmap}.

%We draw inspiration from previous works on IOMMU security \cite{markuze2016true,markuze2018damn}. In that case, dynamic remapping was incompatible with performance and yet extensively used. 
We assert, that the secure and performant solution for networking is a statically mapped memory region dedicated for network I/O. We demonstrate how to leverage a Memory Allocator for I/O (\oursys) approach to provide generic zero-copy support for \sockets without compromising the systems security. 
\oursys consists of a modified Linux kernel 5.4, in which we add I/O pages to the kernels memory management system. These I/O pages are solely used for network I/O, thus insulating I/O memory which is accessible to the user from the rest of the kernels memory. Additionally \oursys has a user space library for managing I/O memory. We also provide a DPDK Poll Mod Driver (PMD) which can support multiple NIC types with a single \oursys PMD.
%We classify the existing solutions into five main categories(Sec .\ref{sec:design}) and show that the statically mapped shared buffer approach has remained relatively unexplored. With most recent techniques (e.g., \cite{xdp,dpdk,rizzo2012netmap,mikelangelo,desendmsg}) focusing instead on dynamic remapping and kernel bypass technologies.

To provide a fully overhead free I/O, we also work to eliminate the cost of system calls. In this case, we adopt the kernel thread approach proposed by FlexSC\cite{flexsc} and exemplified in \texttt{io\_uring}\cite{io-uring}. We show that shared memory buffers coupled with kernel threads, provide a holistic solution for high performance networking.
%We call our system \oursys, a dedicated Memory Allocator for I/O. 

\oursys is potentially beneficial for many VMware products like NSX and Velocloud. Additionally, \oursys is also extendable to ESX guest networking, potentially eliminating the performance gap between SR-IOV NICs and VMXNET3 para-virtual devices.